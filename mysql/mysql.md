# mysql知识点汇总

## mysql基本架构
mysql主要分为server层和存储引擎层。而server层包括了连接器、查询缓存、分析器、优化器、执行器等。

server层包括了mysql大多数的核心功能，以及所有的内置函数(count,sum等)，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

存储引擎层则是负责数据的存储以及读取，支持 InnoDB、MyISAM、Memory 等多个存储引擎（架构模式是插件式的，所以mysql也可以自定义存储引擎来使用）。mysql5.5.5之前默认存储引擎为MyISAM，mysql5.5.5之后则是InnoDB。

其基本架构示意图如下所示：
![](https://img2022.cnblogs.com/blog/901559/202201/901559-20220130171301031-2030452943.jpg)

### server层
#### 连接器
连接器负责跟客户端建立连接、获取权限、维持和管理连接，也就是你用mysql客户端连接工具执行```mysql -u xxx -p xxx```命令。

连接器会校验你的用户名密码是否正确，成功建立连接后连接器会在权限表查询该用户所拥有的权限，之后执行的命令都需要根据此时读取到的权限来校验是否有权执行命令。（在修改用户权限之后，用户需要重新登录权限才能生效的原因就是在建立连接之后读取了账号权限，在断开连接之前都会用该权限来校验）

注意，成功建立连接后（长连接），mysql在执行过程中所申请的内存空间都是由连接对象管理的，这些资源只有在连接对象断开之后才会释放，所以如果连接长期不释放的话（排序、变量这些会占用内存），很容易就会造成内存溢出（客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时）。

长连接不提前释放内存的原因如果这个链接是要复用的，提前释放反而影响性能（因为要重新申请内存），MySQL没法自己决定，所以5.7之后提供了方法让客户端有命令```mysql_reset_connection()```来做

#### 查询缓存
建立连接后，你就可以执行mysql语句了，而在执行```select```语句的时候，会先去查询缓存里找，之前执行过的语句会在查询缓存里以key-value的形式保存起来（key是查询语句，value是查询结果），如果能知道对应的key，则直接返回查询结果，就不需要进行后续查询步骤了（如果命中查询缓存，会在查询缓存返回结果的时候做权限验证，没有权限就不会返回结果）。

不过查询缓存的命中率一般很低，特别是对应更新比较频繁的表，毕竟你一条update语句更新了表的数据，那么对应的缓存数据就全清空了，所以一般不建议使用查询缓存。正因如此，mysql8.0后直接就把查询缓存的整块功能给删掉了（8.0之前的版本只要将参数query_cache_type设置成0就可以不使用查询缓存）

##### query_cache_type
既然提到了这个参数，这里也解释一下：

- 0 关闭查询缓存
- 1 打开查询缓存
- 2 只有select 中明确指定SQL_CACHE才缓存，如：

	```select SQL_CACHE * from test where id = 1```

#### 分析器
如果查询缓存没有命中或者被关闭，那么就会执行sql语句，此时分析器会经过词法分析和语法分析这两个步骤进行处理(图中分析器->查询缓存的意思是，更新完数据后需要失效查询缓存【如果有】)。

##### 词法分析
词法分析就是识别你的sql里面的字符串分表代表什么，比如以下这条语句：

```select * from test where id = 1```

mysql会根据```select```关键字分析出这条语句是查询语句，```from```关键字后面跟着的是表名，```where```关键字后面跟着的就是查询条件。

##### 语法分析
做完词法分析后，mysql就会根据语法规则，检查你这条语句是否有满足sql的语法，表是否存在，列是否存在等，不满足则返回错误提示，如：

```MySQL server version for the right syntax to use near 'xxxx'``` 或者 ```Unknown column 'xxx'```

主要关注```use near```和```Unknown column```，一般这个后面就是提示mysql语句发生错误出现的第一个位置

#### 优化器
经过了分析器之后，检查了sql语句没有语法错误了，在执行之前，还需要优化器对sql进行优化。比如说：

```select * from test where aid = 1 and bid > 2```

test表里分别对aid和bid建立了索引，优化器就要对该条语句选择使用哪个索引。

又比如说:

```select * from test1 as t1 left join test2 as t2 on t1.id = t2.id where t1.aid = 10 and t2.bid = 20```

mysql优化器需要选择这个join语句哪个表作为驱动表(explain结果中，第一行出现的表就是驱动表)，根据驱动表的不同，执行效率会不同。

优化器是一个十分复杂的东西，后文会根据不同的例子，详细的说明优化器的内容，以及结合自身的经验如何优化sql语句。

#### 执行器
经过了语法校验和优化之后，就真正的开始执行sql语句了，在开始执行之前，会先校验权限，看登录用户是否有权执行这条sql（连接器已经取出用户的权限了，在执行器的时候才用来校验权限，既然连接器已经取出了权限，为啥不在优化器之前校验，原因是有些时候，SQL语句要操作的表不只是SQL字面上那些。比如如果有个触发器，得在执行器阶段（过程中）才能确定。优化器阶段前是检测不到的）

以```select * from test where id = 10```为例，如果id上没有索引，那么其执行流程如下所示：

1. 调用InnoDB引擎接口取这个表的第一行，判断id值是不是10，如果不是则跳过，如果是则将这行存在结果集中
2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行
3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端

如果有索引，则是：

1. 调用InnoDB引擎接口取这个表满足条件的第一行
2. 调用引擎接口取“下一行”，直到下一行的id不为10则停止继续取出
3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端

### 存储引擎层
在经过server层连接器，分析器，优化器，执行器等一系列过程之后，最后到达的就是存储引擎层，而存储引擎层主要是负责数据的读写，这里主要介绍的是innodb存储引擎。读就比较简单，找到数据直接返回就可以了，但写话，我们平时执行的update，insert语句，如何保证能成功写入呢？

#### innodb存储引擎的写入
虽说我们更新或者插入数据仅仅只是用一条sql就可以搞定了，看似简单，但是其流程却是经历一系列步骤，其详细流程如下图所示：
![](https://img2022.cnblogs.com/blog/901559/202202/901559-20220201162649518-315247849.png)

##### undo log
innodb每次对数据进行写入或者更新的时候，都会生成一个事务id（递增），根据执行的语句生成一个回滚数据的语句（如：insert语句就会生成delete语句，update语句就会生成重新更新回旧数据的update语句），将这个语句和事务id写入undo log，会得到一个回滚指针，然后更新对应数据行（比如id=2的这行数据）事务id和回滚指针，当事务需要回滚的时候，就可以根据事务回滚指针找到对应的回滚sql，进行旧数据的恢复（回滚操作）。除此之外，undo log 也通过事务id来判断事务的可见性，详情将会在事务隔离详细介绍

##### change buffer
change buffer 是用来优化二级且非唯一索引，且对应的数据页没在内存（buffer pool）中的数据插入和更新上的（简单来说就是优化为批量操作，而且对update操作来说还可以减轻从磁盘加载数据页的性能损耗）。

因为每次插入或者更新，假如对应的数据页不存在内存中，都需要从磁盘中把对应的数据页读取出来，然后再进行数据页的修改操作。

有了change buffer之后，假如遇到数据页不在内存中，且不影响数据的唯一性的情况下，那么可以先把数据写入到change buffer中，等对应的数据页被访问从而加载到内存中的时候，再把change buffer中对应的新数据修改到内存页中（称为merge操作），亦或者每隔一段时间，由后台线程定期的触发merge操作，这样就可以把若干对同一页面的更新缓存起来做，合并为一次性更新操作。（减少IO，转随机IO为顺序IO,这样可以避免随机IO带来性能损耗，提高数据库的写性能）

- 为什么数据页在内存中，不使用change buffer

	change buffer 其中一点优化就是为了不要每次更新都把数据从磁盘加载到内存中（对应的数据页不存在内存的情况下），既然数据页已经在内存中了，那么直接改就行了。

- 为什么数据唯一的时候，不使用change buffer

	因为每次唯一索引数据更新或者插入的时候，都需要判断数据的唯一性，不存在这个数据才能更新或者插入，在校验唯一性的过程中，不管怎样都需要把对应的数据页加载到内存中，既然数据页已经在内存中了，那么直接改就行了。

##### redo log和binlog
redo log和bin log 是mysql两个十分重要的日志，其中redo log是innodb特有的，用来实现crash-safe，而binlog是server层特有的日志，所以无论哪个存储引擎都能够使用。

###### redo log
如果数据每次在每次更新或者插入的时候，都直接写磁盘持久化的话，那么这样会带来io的性能损耗，为了优化这点，innodb使用了一种WAL技术（Write-Ahead Logging），也就是在数据落盘之前，innodb会先写日志，再写磁盘。

具体来说，就是更新和插入的数据，innodb只要把buffer pool的数据更新了，写完了redo log，就认为数据的更新已经完成了（当然还要写bin log，但是这是server层做的事）。同时，innodb一般会在系统比较空闲的时候，再把redo log的操作记录批量更新到磁盘里，这样做的好处就是减少了io，把随机写改成了顺序写。

除此之外，redo log还能保证数据库发生异常重启，之前提交的记录都不会丢失，因为只要写到redo log后，发生了异常重启只要把重新把磁盘中redo log的数据重新加载回内存即可（crash-safe）。

当然，redo log的内存不是无限大的，而是固定大小（参数innodb_log_file_size【每个redo log文件大小】 innodb_mirrored_log_groups【有多少个redo log文件，但5.7该参数就被废弃了】），而且是个环状结构，其结构如下图所示：
![](https://img2022.cnblogs.com/blog/901559/202202/901559-20220202230337968-1594751133.png)

- write pos：当前记录写到的位置
- check point：开始回收内存的位置

所以write pos和check point就是redo log内存所剩空间，如果write pos追上了check point，那么innodb就无法继续执行数据更新，只能先把redo log中的数据持久化落盘，回收内存，推进check point位置空出剩余内存后，才能继续执行更新操作。

###### binlog
用于记录mysql执行插入和更新操作的相关语句，用于数据备份和主从复制。

###### redo log和binlog的不同点
- redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。

- redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。

- redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

###### 二阶段提交
9~13的执行过程，其实就是二阶段提交，其过程细化后，如下图所示：
![](https://img2022.cnblogs.com/blog/901559/202202/901559-20220202233717482-1824718105.png)

1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。

2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。

3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。

4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。

5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。

其中，如果在执行的过程中，服务器down掉了，那个分为3种情况进行分析：

- 如果redo log处在prepare阶段，但还没写binlog，那么恢复之后就会执行数据的回滚操作

- 如果redo log处在prepare阶段，已经写了binlog，那么恢复之后只要在binlog找到记录，就会把redo log改为commit状态

- 如果redo log处在commit状态， 那么说明binlog肯定有对应的数据，那么直接提交事务即可

为什么要搞这么复杂，其实就是为了保证redo log和binlog数据的一致性。

试想下，如果binlog和redo log数据不一致的话，那么数据一致性肯定会遭到破坏，比如：
- binlog 多了一条update id = 2的操作，而redo log没有，那么从库的数据肯定和主库不一直，而且用binlog在恢复数据的时候，也会莫名其妙多了一条update id = 2

- 反之，redo log有而binlog没有，那么从库肯定会丢了这条数据，而恢复数据的时候，也会丢掉这条数据

###### 双1参数介绍
双1参数指的是innodb_flush_log_at_trx_commit和sync_binlog，分别表示每次事务的redo log和binlog持久化到磁盘的策略

- innodb_flush_log_at_trx_commit

	- 0
		每次事务提交时都只是把redo log留在redo log buffer中
	
	- 1
		每次事务提交时都将redo log直接持久化到磁盘
	
	- 2
		每次事务提交时都只是把redo log写到page cache（innodb有一个后台线程，会每秒调用write把redo log buffer写到page cache，再调用fsync持久化到磁盘）

- sync_binlog

	- 0
		每次提交事务都只write，不fsync
	
	- 1
		每次提交事务都会执行fsync
	
	- N（N>1）
		每次提交事务都write，但累积N个事务后才fsync

##### double write
提供可靠性（就是从page cache中持久化到磁盘过程提供可靠性，因为mysql页是16k，而操作系统磁盘写入最小单位是4k，所以刷盘的时候只能4k4k地写）

innoDB存储引擎正在写入某个页到表中，突然发生了宕机（比如16KB的页，只写了4KB），通过重做日志中的记录进行恢复的时候（重新执行），因为页已经发生了损坏（写了4KB的数据），再重新对该页操作是没意义的

所以引入了双写，在对缓冲池的脏页进行刷新时，先记录原先该页的数据，当写入失效发生的时候，先恢复该页原来的数据，在重新通过重做日志重新执行一次操作



## 事务隔离
### 事务的四个特性
分别是原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability），即ACID

### 隔离级别
一共4种，分别是读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）

- 读未提交
	一个事务还没提交时，它做的变更就能被别的事务看到

- 读提交
	一个事务提交之后，它做的变更才会被其他事务看到

- 可重复读
	一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。

- 串行化
	对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

#### 读提交和可重复读区别
假如说有事务A和事务B，事务A还在执行过程中，而事务B则是已经执行完成，那么在读提交的隔离级别下，事务A可以即时获取事务B更新的数据，而在可重复读隔离级别下，事务A只能获取开始事务时刻的数据。其执行流程如下图所示：

![](https://img2022.cnblogs.com/blog/901559/202202/901559-20220203164655076-2004289873.png)

- 读未提交

	V1 的值就是 2。这时候事务 B 虽然还没有提交，但是结果已经被 A 看到了。因此，V2、V3 也都是 2

- 读提交

	V1 是 1，V2 的值是 2。事务 B 的更新在提交后才能被 A 看到。所以， V3 的值也是 2

- 可重复读

	V1、V2 是 1，V3 是 2。之所以 V2 还是 1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的

- 串行化

	V1、V2 是 1，V3 是 2。之所以 V2 还是 1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的

### mvcc
#### 可见性视图
- 可重复读

	在事务启动时创建的，整个事务存在期间都用这个视图

- 读提交

	在每个 SQL 语句开始执行的时候创建的

- 读未提交

	不使用可见性视图，直接返回记录上的最新值

- 串行化

	不使用可见性实体，用加锁的方式来避免并行访问

### 查询当前长事务语句
```select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60```


## 索引
### 索引常见模型
#### 哈希表
哈希表是一种键值对的存储数据结构（key-value），key用哈希函数换算后得到的下标，然后把数据放在对应下标的value数组中。

如果出现哈希冲突，就使用拉链法，用链表存储在哈希出相同下标值的多个数据

因为数据不是有序的，所以插入数据，等值查询很快，但是哈希索引做区间查询只能全扫描，效率很慢，因此哈希索引一般用于比如Memcached及其他一些NoSQL引擎

### 有序数组
有序数组在等值查询和范围查询场景中的性能就都很好，但是对插入或者更新数据，效率就非常慢，因为每次插入或者更新数据，都需要挪动后面所有的记录。

## 锁

